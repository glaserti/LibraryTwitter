{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function call of __FnFMining__ wraps up all the helper functions in this Notebook.\n",
      "It's using the IDs harvested in the last Notebook and saved in the NetWork-files to collect data about these IDs.\n",
      "\n",
      "The function takes as input the twitterfiles of each category (NatBibTwitter.csv etc.), opens for each library in this file the corresponding NetWork-[datestamp]-file and returns\n",
      "\n",
      "   - for each library\n",
      "      - a csv-file with the Friends-IDs and Data (if the library is following other accounts; there are some libraries which are actually not following)\n",
      "      - a csv-file with the Followers-IDs and Data\n",
      "\n",
      "   - a list of the files for each library category, called NatBib_Files.txt etc.\n",
      "   - also an error message for user IDs which could not be accessed. This is, most of the time, because the accounts were deleted. One could check these IDs via a service as e.g. http://tweeterid.com.\n",
      "   \n",
      "The Friends and Followers files contain List of Dictionaries (LoD) with the keys: friends_description, friends_user_id, friends_location, friends_screen_name, and followers_description, followers_user_id, followers_location, followers_screen_name respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Code from MTSW 2Ed.\n",
      "# cf. https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition\n",
      "\n",
      "import twitter\n",
      "\n",
      "def oauth_login():\n",
      "    # XXX: Go to http://twitter.com/apps/new to create an app and get values\n",
      "    # for these credentials that you'll need to provide in place of these\n",
      "    # empty string values that are defined as placeholders.\n",
      "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
      "    # on Twitter's OAuth implementation.\n",
      "    \n",
      "    CONSUMER_KEY = \n",
      "    CONSUMER_SECRET =\n",
      "    OAUTH_TOKEN = \n",
      "    OAUTH_TOKEN_SECRET =    \n",
      "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
      "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
      "    \n",
      "    twitter_api = twitter.Twitter(auth=auth)\n",
      "    return twitter_api\n",
      "\n",
      "# Sample usage\n",
      "twitter_api = oauth_login()    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#importing libraries\n",
      "import json   #for pretty printing\n",
      "import time   #for calculating Tweets per day\n",
      "import operator #for sorting dictionaries\n",
      "from collections import Counter #for turning lists to dictionaries etc.\n",
      "from prettytable import PrettyTable   #for pretty printing in a table\n",
      "\n",
      "\n",
      "# helper function Prettyprint taken from MTSW 2Ed.\n",
      "\n",
      "def prettyPrint(Sp_1, Sp_2, counted_list_of_tuples):\n",
      "    ptLang = PrettyTable(field_names=[Sp_1, Sp_2])\n",
      "    [ptLang.add_row(kv) for kv in counted_list_of_tuples]\n",
      "    ptLang.align[Sp_1], ptLang.align[Sp_2] = 'l', 'r'\n",
      "    print ptLang\n",
      "    \n",
      "# helper function: safe the results as a csv-file\n",
      "\n",
      "#import & export CSV\n",
      "import csv\n",
      "\n",
      "def impCSV(input_file):\n",
      "    '''\n",
      "    input_file = csv with keys: \"URL\", \"Twitter\"\n",
      "    output = list of dictionaries\n",
      "    '''\n",
      "    f = open(input_file, 'r')\n",
      "    d = csv.DictReader(f)\n",
      "    LoD = []   # list of dictionaries\n",
      "    for row in d:\n",
      "        LoD.append(row)\n",
      "    f.close()\n",
      "    return LoD\n",
      "\n",
      "def exp2CSV(listOfDict, filename):\n",
      "    '''\n",
      "    arguments = list of dictionaries, filename\n",
      "    output = saves file to cwd (current working directory)\n",
      "    '''\n",
      "    outputfile = filename\n",
      "    keyz = listOfDict[0].keys()\n",
      "    f = open(outputfile,'w')\n",
      "    dict_writer = csv.DictWriter(f,keyz)\n",
      "    dict_writer.writer.writerow(keyz)\n",
      "    dict_writer.writerows(listOfDict)\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Both functions from MTSW 2 Ed.\n",
      "\n",
      "import sys\n",
      "from urllib2 import URLError\n",
      "from httplib import BadStatusLine\n",
      "\n",
      "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
      "    \n",
      "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
      "    # value for wait_period if the problem is a 500 level error. Block until the\n",
      "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
      "    # for 401 and 404 errors, which requires special handling by the caller.\n",
      "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
      "    \n",
      "        if wait_period > 3600: # Seconds\n",
      "            print >> sys.stderr, 'Too many retries. Quitting.'\n",
      "            raise e\n",
      "    \n",
      "        # See https://dev.twitter.com/docs/error-codes-responses for common codes\n",
      "    \n",
      "        if e.e.code == 401:\n",
      "            print >> sys.stderr, 'Encountered 401 Error (Not Authorized)'\n",
      "            return None\n",
      "        elif e.e.code == 404:\n",
      "            print >> sys.stderr, 'Encountered 404 Error (Not Found)'\n",
      "            return None\n",
      "        elif e.e.code == 429: \n",
      "            print >> sys.stderr, 'Encountered 429 Error (Rate Limit Exceeded)'\n",
      "            if sleep_when_rate_limited:\n",
      "                print >> sys.stderr, \"Retrying in 15 minutes...ZzZ...\"\n",
      "                sys.stderr.flush()\n",
      "                time.sleep(60*15 + 5)\n",
      "                print >> sys.stderr, '...ZzZ...Awake now and trying again.'\n",
      "                return 2\n",
      "            else:\n",
      "                raise e # Caller must handle the rate limiting issue\n",
      "        elif e.e.code in (500, 502, 503, 504):\n",
      "            print >> sys.stderr, 'Encountered %i Error. Retrying in %i seconds' % \\\n",
      "                (e.e.code, wait_period)\n",
      "            time.sleep(wait_period)\n",
      "            wait_period *= 1.5\n",
      "            return wait_period\n",
      "        else:\n",
      "            raise e\n",
      "\n",
      "    # End of nested helper function\n",
      "    \n",
      "    wait_period = 2 \n",
      "    error_count = 0 \n",
      "\n",
      "    while True:\n",
      "        try:\n",
      "            return twitter_api_func(*args, **kw)\n",
      "        except twitter.api.TwitterHTTPError, e:\n",
      "            error_count = 0 \n",
      "            wait_period = handle_twitter_http_error(e, wait_period)\n",
      "            if wait_period is None:\n",
      "                return\n",
      "        except URLError, e:\n",
      "            error_count += 1\n",
      "            time.sleep(wait_period)\n",
      "            wait_period *= 1.5\n",
      "            print >> sys.stderr, \"URLError encountered. Continuing.\"\n",
      "            if error_count > max_errors:\n",
      "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
      "                raise\n",
      "        except BadStatusLine, e:\n",
      "            error_count += 1\n",
      "            time.sleep(wait_period)\n",
      "            wait_period *= 1.5\n",
      "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
      "            if error_count > max_errors:\n",
      "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
      "                raise\n",
      "\n",
      "# See https://dev.twitter.com/docs/api/1.1/get/users/lookup for \n",
      "# twitter_api.users.lookup\n",
      "\n",
      "\n",
      "def get_user_profile(twitter_api, screen_names=None, user_ids=None):\n",
      "   \n",
      "    # Must have either screen_name or user_id (logical xor)\n",
      "    assert (screen_names != None) != (user_ids != None), \\\n",
      "    \"Must have screen_names or user_ids, but not both\"\n",
      "    \n",
      "    items_to_info = {}\n",
      "\n",
      "    items = screen_names or user_ids\n",
      "    \n",
      "    while len(items) > 0:\n",
      "\n",
      "        # Process 100 items at a time per the API specifications for /users/lookup.\n",
      "        # See https://dev.twitter.com/docs/api/1.1/get/users/lookup for details.\n",
      "        \n",
      "        items_str = ','.join([str(item) for item in items[:100]])\n",
      "        items = items[100:]\n",
      "\n",
      "        if screen_names:\n",
      "            response = make_twitter_request(twitter_api.users.lookup, \n",
      "                                            screen_name=items_str)\n",
      "        else: # user_ids\n",
      "            response = make_twitter_request(twitter_api.users.lookup, \n",
      "                                            user_id=items_str)\n",
      "    \n",
      "        for user_info in response:\n",
      "            if screen_names:\n",
      "                items_to_info[user_info['screen_name']] = user_info\n",
      "            else: # user_ids\n",
      "                items_to_info[user_info['id']] = user_info\n",
      "\n",
      "    return items_to_info\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lookUpProfilesFriends(listOfIDs):\n",
      "    ''' \n",
      "    input: list of IDs of Friends or Followers\n",
      "    output: list dictionaries with keys 'user_id', 'screen_name', 'location', 'description'\n",
      "    '''\n",
      "    LoD = []\n",
      "    errorIDs = []\n",
      "    \n",
      "    profiles = get_user_profile(twitter_api, user_ids=listOfIDs)\n",
      "    try:\n",
      "        for e in listOfIDs:\n",
      "            infoDic = {}\n",
      "            infoDic['friends_user_id'] = e\n",
      "            infoDic['friends_screen_name'] = profiles[e]['screen_name']\n",
      "            infoDic['friends_location'] = (profiles[e]['location']).encode('utf-8')\n",
      "            infoDic['friends_description'] = (profiles[e]['description']).encode('utf-8')\n",
      "            LoD.append(infoDic)\n",
      "    except:\n",
      "        errorIDs.append(e)\n",
      "    if len(errorIDs) > 0:\n",
      "        print\n",
      "        print 'Error for these IDs:', errorIDs\n",
      "        print\n",
      "    return LoD\n",
      "\n",
      "\n",
      "def lookUpProfilesFollowers(listOfIDs):\n",
      "    ''' \n",
      "    input: list of IDs of Friends or Followers\n",
      "    output: list dictionaries with keys 'user_id', 'screen_name', 'location', 'description'\n",
      "    '''\n",
      "    LoD = []\n",
      "    errorIDs = []\n",
      "    profiles = get_user_profile(twitter_api, user_ids=listOfIDs)\n",
      "    \n",
      "    try:\n",
      "        for e in listOfIDs:\n",
      "            infoDic = {}\n",
      "            infoDic['followers_user_id'] = e\n",
      "            infoDic['followers_screen_name'] = profiles[e]['screen_name']\n",
      "            infoDic['followers_location'] = (profiles[e]['location']).encode('utf-8')\n",
      "            infoDic['followers_description'] = (profiles[e]['description']).encode('utf-8')\n",
      "            LoD.append(infoDic)\n",
      "    except:\n",
      "        errorIDs.append(e)\n",
      "    if len(errorIDs) > 0:\n",
      "        print 'Error for these IDs:', errorIDs\n",
      "    \n",
      "    return LoD\n",
      "\n",
      "\n",
      "def wrapLookUp(dictOfFnFs):\n",
      "    '''\n",
      "    input: dict of FnFs of a lib (with keys 'followers_ids', 'friends_ids', 'screen_name' (of the lib)\n",
      "    output: a list of filenames\n",
      "    saves two files: <twitterhandel>_Friends_<datestamp>.csv and <twitterhandel>_Followers_<datestamp>.csv\n",
      "    '''\n",
      "    f1 = dictOfFnFs['friends_ids']\n",
      "    f2 = dictOfFnFs['followers_ids']\n",
      "    \n",
      "    #in case the list is converted to a str\n",
      "    if type(f1) == str and f1 != '[]':\n",
      "        f11 = f1.strip('[]')\n",
      "        f1 = [int(s) for s in f11.split(',')]\n",
      "    else:\n",
      "        pass\n",
      "    if type(f2) == str and f2 != '[]':\n",
      "        f21 = f2.strip('[]')\n",
      "        f2 = [int(s) for s in f21.split(',')]\n",
      "    else:\n",
      "        pass\n",
      "    \n",
      "    if len(f1) > 0 and type(f1) == list:\n",
      "        friends = lookUpProfilesFriends(f1)\n",
      "    else:\n",
      "        friends = []\n",
      "    if len(f2) > 0 and type(f2) == list:\n",
      "        followers = lookUpProfilesFollowers(f2)\n",
      "    else:\n",
      "        followers = []\n",
      "            \n",
      "    #creating the filename of the csv with current datestamp \n",
      "    import datetime\n",
      "    datestamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
      "    filename_friends = dictOfFnFs['screen_name'] + '_Friends_' + datestamp + '.csv'\n",
      "    filename_followers = dictOfFnFs['screen_name'] + '_Followers_' + datestamp + '.csv'\n",
      "    LoFilenames = [] # [filename_friends, filename_followers]\n",
      "    \n",
      "    #export as CSV to CWD\n",
      "    if len(friends) > 0:\n",
      "        exp2CSV(friends, filename_friends)\n",
      "        LoFilenames.append(filename_friends)\n",
      "    if len(followers) > 0:\n",
      "        exp2CSV(followers, filename_followers)\n",
      "        LoFilenames.append(filename_followers)\n",
      "  \n",
      "    return LoFilenames\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def FnFMining(Twitterfile, datestamp):\n",
      "    '''\n",
      "    input: the NatBibTwitter.csv etc. filenames and the datestamp of the'_NetWork_2014-03-11.csv' file.\n",
      "    (the library Twitter name will be added).\n",
      "    '''\n",
      "    import pickle                      # for saving the list to a file\n",
      "    \n",
      "    f = impCSV(Twitterfile)\n",
      "    listOfFilenames = []\n",
      "    for e in f:\n",
      "        n = e['Twitter']                # get Twitter handel of the library\n",
      "        filename = n + '_NetWork_' + datestamp + '.csv' # create the filename for the library\n",
      "        print filename\n",
      "        b = impCSV(filename)            # import this file\n",
      "        p = wrapLookUp(b[0])            # get description etc. for the FnFs of the library\n",
      "        \n",
      "        print p                        # print the filenames for each library\n",
      "        listOfFilenames += p\n",
      "\n",
      "    # for saving the list to a file    \n",
      "    filename2 = Twitterfile[:-11] + '_Files.txt'   # creating a filename like UniBibFiles.txt\n",
      "    print filename2\n",
      "    with open(filename2, 'wb') as f:\n",
      "        pickle.dump(listOfFilenames, f)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Function Calls"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "National Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FnFMining('NatBibTwitter2.csv', '2014-04-06')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "bsb_muenchen_NetWork_2014-04-06.csv\n",
        "['bsb_muenchen_Friends_2014-04-07.csv', 'bsb_muenchen_Followers_2014-04-07.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dnb_aktuelles_NetWork_2014-04-06.csv\n",
        "['dnb_aktuelles_Friends_2014-04-07.csv', 'dnb_aktuelles_Followers_2014-04-07.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sbb_news_NetWork_2014-04-06.csv\n",
        "Error for these IDs:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [12456992]\n",
        "['sbb_news_Followers_2014-04-07.csv']\n",
        "NatBibT_Files.txt\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "University Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FnFMining('UniBibTwitter2.csv', '2014-04-06')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ub_oldenburg_NetWork_2014-04-06.csv\n",
        "['ub_oldenburg_Friends_2014-04-06.csv', 'ub_oldenburg_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hsubib_NetWork_2014-04-06.csv\n",
        "['hsubib_Friends_2014-04-06.csv', 'hsubib_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ubhumboldtuni_NetWork_2014-04-06.csv\n",
        "['ubhumboldtuni_Friends_2014-04-06.csv', 'ubhumboldtuni_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kitbibliothek_NetWork_2014-04-06.csv\n",
        "['kitbibliothek_Friends_2014-04-06.csv', 'kitbibliothek_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kizuulm_NetWork_2014-04-06.csv\n",
        "['kizuulm_Friends_2014-04-06.csv', 'kizuulm_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "subugoe_NetWork_2014-04-06.csv\n",
        "Error for these IDs:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [50072429]\n",
        "['subugoe_Friends_2014-04-06.csv', 'subugoe_Followers_2014-04-06.csv']\n",
        "ubbochum_NetWork_2014-04-06.csv\n",
        "['ubbochum_Friends_2014-04-06.csv', 'ubbochum_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slubdresden_NetWork_2014-04-06.csv\n",
        "Error for these IDs:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [74824233]\n",
        "['slubdresden_Friends_2014-04-06.csv', 'slubdresden_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "elibbremen_NetWork_2014-04-06.csv\n",
        "['elibbremen_Friends_2014-04-06.csv', 'elibbremen_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabihh_NetWork_2014-04-06.csv\n",
        "['stabihh_Friends_2014-04-06.csv', 'stabihh_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Encountered 429 Error (Rate Limit Exceeded)\n",
        "Retrying in 15 minutes...ZzZ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ub_tu_berlin_NetWork_2014-04-06.csv\n",
        "['ub_tu_berlin_Friends_2014-04-06.csv', 'ub_tu_berlin_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tubhh_NetWork_2014-04-06.csv\n",
        "['tubhh_Friends_2014-04-06.csv', 'tubhh_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ulbbonn_NetWork_2014-04-06.csv\n",
        "['ulbbonn_Friends_2014-04-06.csv', 'ulbbonn_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ubbayreuth_info_NetWork_2014-04-06.csv\n",
        "['ubbayreuth_info_Friends_2014-04-06.csv', 'ubbayreuth_info_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ub_bi_NetWork_2014-04-06.csv\n",
        "['ub_bi_Friends_2014-04-06.csv', 'ub_bi_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "unibib_bs_NetWork_2014-04-06.csv\n",
        "['unibib_bs_Friends_2014-04-06.csv', 'unibib_bs_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ub_wue_NetWork_2014-04-06.csv\n",
        "['ub_wue_Friends_2014-04-06.csv', 'ub_wue_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "unibib_NetWork_2014-04-06.csv\n",
        "['unibib_Friends_2014-04-06.csv', 'unibib_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "...ZzZ...Awake now and trying again.\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ubdue_NetWork_2014-04-06.csv\n",
        "['ubdue_Friends_2014-04-06.csv', 'ubdue_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ub_fau_NetWork_2014-04-06.csv\n",
        "['ub_fau_Friends_2014-04-06.csv', 'ub_fau_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tibub_NetWork_2014-04-06.csv\n",
        "['tibub_Friends_2014-04-06.csv', 'tibub_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ubkassel_NetWork_2014-04-06.csv\n",
        "['ubkassel_Friends_2014-04-06.csv', 'ubkassel_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ubleipzig_NetWork_2014-04-06.csv\n",
        "Error for these IDs:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [166163038]\n",
        "['ubleipzig_Friends_2014-04-06.csv', 'ubleipzig_Followers_2014-04-06.csv']\n",
        "ubmainz_NetWork_2014-04-06.csv\n",
        "['ubmainz_Friends_2014-04-06.csv', 'ubmainz_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "unibib_mr_NetWork_2014-04-06.csv\n",
        "['unibib_mr_Friends_2014-04-06.csv', 'unibib_mr_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ubreg_NetWork_2014-04-06.csv\n",
        "Error for these IDs:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [334516554]\n",
        "['ubreg_Friends_2014-04-06.csv', 'ubreg_Followers_2014-04-06.csv']\n",
        "zbsport_NetWork_2014-04-06.csv\n",
        "['zbsport_Friends_2014-04-06.csv', 'zbsport_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "UniBibT_Files.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Public Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FnFMining('OeBibTwitter2.csv', '2014-04-06')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "stb_bielefeld_NetWork_2014-04-06.csv\n",
        "Error for these IDs:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [23788414]\n",
        "['stb_bielefeld_Friends_2014-04-06.csv', 'stb_bielefeld_Followers_2014-04-06.csv']\n",
        "stabi_bremen_NetWork_2014-04-06.csv\n",
        "['stabi_bremen_Friends_2014-04-06.csv', 'stabi_bremen_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stbessen_NetWork_2014-04-06.csv\n",
        "['stbessen_Friends_2014-04-06.csv', 'stbessen_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stbibkoeln_NetWork_2014-04-06.csv\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error for these IDs: [171132336]\n",
        "\n",
        "['stbibkoeln_Friends_2014-04-06.csv', 'stbibkoeln_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stadtbueduedorf_NetWork_2014-04-06.csv\n",
        "['stadtbueduedorf_Friends_2014-04-06.csv', 'stadtbueduedorf_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hoeb4u_NetWork_2014-04-06.csv\n",
        "['hoeb4u_Friends_2014-04-06.csv', 'hoeb4u_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bibliothek_wit_NetWork_2014-04-06.csv\n",
        "['bibliothek_wit_Friends_2014-04-06.csv', 'bibliothek_wit_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mediothek_NetWork_2014-04-06.csv\n",
        "['mediothek_Friends_2014-04-06.csv', 'mediothek_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabi_erlangen_NetWork_2014-04-06.csv\n",
        "['stabi_erlangen_Friends_2014-04-06.csv', 'stabi_erlangen_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabifr_NetWork_2014-04-06.csv\n",
        "['stabifr_Friends_2014-04-06.csv', 'stabifr_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabigoe_NetWork_2014-04-06.csv\n",
        "['stabigoe_Friends_2014-04-06.csv', 'stabigoe_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stbneuss_NetWork_2014-04-06.csv\n",
        "['stbneuss_Friends_2014-04-06.csv', 'stbneuss_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stbsalzgitter_NetWork_2014-04-06.csv\n",
        "['stbsalzgitter_Friends_2014-04-06.csv', 'stbsalzgitter_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabiso_NetWork_2014-04-06.csv\n",
        "['stabiso_Friends_2014-04-06.csv', 'stabiso_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sbchemnitz_NetWork_2014-04-06.csv\n",
        "['sbchemnitz_Friends_2014-04-06.csv', 'sbchemnitz_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabiguetersloh_NetWork_2014-04-06.csv\n",
        "['stabiguetersloh_Friends_2014-04-06.csv', 'stabiguetersloh_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabi_mannheim_NetWork_2014-04-06.csv\n",
        "['stabi_mannheim_Friends_2014-04-06.csv', 'stabi_mannheim_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stadtbibliothek_NetWork_2014-04-06.csv\n",
        "['stadtbibliothek_Friends_2014-04-06.csv', 'stadtbibliothek_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stadtbibmg_NetWork_2014-04-06.csv\n",
        "['stadtbibmg_Friends_2014-04-06.csv', 'stadtbibmg_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "URLError encountered. Continuing.\n",
        "URLError encountered. Continuing."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "buecherei_ms_NetWork_2014-04-06.csv\n",
        "['buecherei_ms_Friends_2014-04-06.csv', 'buecherei_ms_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stabuewuerzburg_NetWork_2014-04-06.csv\n",
        "['stabuewuerzburg_Friends_2014-04-06.csv', 'stabuewuerzburg_Followers_2014-04-06.csv']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "OeBibT_Files.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}