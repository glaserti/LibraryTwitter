{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Basic Statistics on the Twitter Accounts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, some basic statistics for the Twitter Accounts of the given groups of libraries (i.e. National libraries, University libraries, Public libraries) will be collected.\n",
      "\n",
      "The functions will return a list of dictionaries and save it as a CSV to the cwd.\n",
      "\n",
      "The dictionaries have as keys: \n",
      "\n",
      "- 'created_at' ( = the Twitter Time Stamp),\n",
      "- 'created_at_sec' ( = the date in seconds (from 1970-01-01), \n",
      "- 'days' (= the number of days since created_at), \n",
      "- 'days_since_last_tweet', \n",
      "- 'followers_count', \n",
      "- 'friends_count', \n",
      "- 'id_str' ( = the Twitter ID as a string), \n",
      "- 'location' ( = if a location is given in the description of the account), \n",
      "- 'screen_name' ( = the Twitter handle/username), \n",
      "- 'statuses_count' ( = Nr. of Tweets), \n",
      "- 'tweets_per_day', \n",
      "- 'tweets_per_year'\n",
      "\n",
      "Finally, there is a Report section, in which an overview is provided. For each library group will be printed out:\n",
      "\n",
      "- The number of libraries,\n",
      "- the median of the groups' Tweets per day,\n",
      "- the oldest and latest library @ Twitter with their Tweets per day ratio,\n",
      "- a list of no longer actively tweeting libraries\n",
      "- the libraries with the most and least Tweets and\n",
      "- a summary for each library.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Function definitions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# authenticating @ Twitter\n",
      "\n",
      "# Function definition taken from Mining the Social Web, 2. Ed.\n",
      "# cf. https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition\n",
      "\n",
      "'''\n",
      "Go to http://dev.twitter.com/apps/new to create an app and get values\n",
      "for these credentials, which you'll need to provide in place of these\n",
      "empty string values that are defined as placeholders.\n",
      "See https://dev.twitter.com/docs/auth/oauth for more information \n",
      "on Twitter's OAuth implementation.\n",
      "'''\n",
      "    \n",
      "#importing libraries\n",
      "import twitter\n",
      "    \n",
      "CONSUMER_KEY = \n",
      "CONSUMER_SECRET =\n",
      "OAUTH_TOKEN = \n",
      "OAUTH_TOKEN_SECRET = \n",
      "\n",
      "    \n",
      "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
      "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
      "twitter_api = twitter.Twitter(auth=auth)\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import & export CSV\n",
      "import csv\n",
      "\n",
      "def impCSV(input_file):\n",
      "    '''\n",
      "    input_file = csv with keys: \"URL\", \"Twitter\"\n",
      "    output = list of dictionaries\n",
      "    '''\n",
      "    f = open(input_file, 'r')\n",
      "    d = csv.DictReader(f)\n",
      "    LoD = []   # list of dictionaries\n",
      "    for row in d:\n",
      "        LoD.append(row)\n",
      "    f.close()\n",
      "    return LoD\n",
      "\n",
      "def exp2CSV(listOfDict, filename):\n",
      "    '''\n",
      "    arguments = list of dictionaries, filename\n",
      "    output = saves file to cwd (current working directory)\n",
      "    '''\n",
      "    #creating the filename of the csv with current datestamp \n",
      "    import datetime\n",
      "    datestamp = datetime.datetime.now().strftime('%Y-%m-%d')    \n",
      "    outputfile = filename[:-4]+ '_' + datestamp + '.csv'\n",
      "    keyz = listOfDict[0].keys()\n",
      "    f = open(outputfile,'w')\n",
      "    dict_writer = csv.DictWriter(f,keyz)\n",
      "    dict_writer.writer.writerow(keyz)\n",
      "    dict_writer.writerows(listOfDict)\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################\n",
      "#                                 #\n",
      "#  Functions for the Data Mining  #\n",
      "#                                 #\n",
      "###################################\n",
      "\n",
      "\n",
      "#importing libraries\n",
      "import json                           #for pretty printing\n",
      "import time                           #for calculating Tweets per day\n",
      "import operator                       #for sorting dictionaries\n",
      "from collections import Counter       #for turning lists to dictionaries etc.\n",
      "from prettytable import PrettyTable   #for pretty printing in a table\n",
      "\n",
      "\n",
      "# getting the ListOfScreenNames\n",
      "def getLoSN(csvFile):\n",
      "    '''\n",
      "    input = csv filename of list of dictionaries with a key \"Twitter\" (where the Screenname is saved)\n",
      "    returns a list of tuples with t[0] = libLocation, t[1] = Twitter screenname\n",
      "    '''\n",
      "    LoD = impCSV(csvFile)\n",
      "    ListOfScreenNamesLocationTuples = []\n",
      "    for i in LoD:\n",
      "        ListOfScreenNamesLocationTuples.append((i['Ort'], i['Twitter']))\n",
      "    return ListOfScreenNamesLocationTuples\n",
      "\n",
      "\n",
      "\n",
      "#getting basic infos for a given account incl. last status update\n",
      "# users.lookup = max. 100 Anfragen pro Session! Not a problem in this section of the queries.\n",
      "def AccountInfo(L):\n",
      "    '''\n",
      "    input = list of tuples with str of screen_names and location\n",
      "    output = list of tuples with t[0] = libLocation, t[1] = lists of dictionaries\n",
      "    '''\n",
      "    outputList = []\n",
      "    errorList = []   #implementation of error checking via \"try\" or something like that!\n",
      "    for n in L:\n",
      "        search_results = twitter_api.users.lookup(screen_name=n[1])\n",
      "        outputList.append((n[0], search_results))\n",
      "    return outputList\n",
      "\n",
      "\n",
      "\n",
      "# getting some basic stats for the screen_names\n",
      "def baseStats(AccountInfoList):\n",
      "    '''\n",
      "    input = return list from AccountInfo(L)\n",
      "    output: list of dictionaries with screenName, UserID, nrOfFollowers, nrOfFriends, \n",
      "    nrOfStatusUpdates, tweetsSince, tweetsPerDay, and tweetsPerYear\n",
      "    '''\n",
      "    AccountInfoList[1]\n",
      "    baseStatsList = []\n",
      "    for e in range(len(AccountInfoList)):\n",
      "        newDict = {}   #creating a new dictionary for each account\n",
      "        screenName = AccountInfoList[e][1][0]['screen_name'].lower()  # cf. @ Notebook 3 - Twitter CSV files\n",
      "        UserID = AccountInfoList[e][1][0]['id_str'].encode('utf-8')\n",
      "        nrOfFollowers = AccountInfoList[e][1][0]['followers_count']   #How many Followers?\n",
      "        nrOfFriends = AccountInfoList[e][1][0]['friends_count']   #How many Following/Friends?\n",
      "        nrOfStatusUpdates = AccountInfoList[e][1][0]['statuses_count']\n",
      "        tweetsSince = AccountInfoList[e][1][0]['created_at'].encode('utf-8')\n",
      "        #new in Dict:\n",
      "        DateOfLastTweet = AccountInfoList[e][1][0]['status']['created_at'].encode('utf-8')\n",
      "        \n",
      "        #normalizing the location\n",
      "        \n",
      "        '''\n",
      "        # This code is only necessary if the Twitter location is used instead of the DBS location\n",
      "        # location = AccountInfoList[e][1][0]['location'].encode('utf-8')   #get the location (in case the screen_name isn't sufficient)\n",
      "        # list of words to remove from the location's description (Bundesl\u00e4nder & Country)\n",
      "        removeWords = ['Deutschland', 'Germany', 'Baden-W\u00fcrttemberg', 'Bayern', 'Brandenburg', 'Hessen', 'Mecklenburg-Vorpommern', \n",
      "              'Niedersachsen', 'Nordrhein-Westfalen', 'Rheinland-Pfalz', 'Saarland', 'Sachsen', \n",
      "              'Sachsen-Anhalt', 'Schleswig-Holstein','Th\u00fcringen'] #ausser 'Berlin', 'Bremen', 'Hamburg'!\n",
      "\n",
      "        #normalizing location (lowercase, stripping of Germany etc.) (\"Oldenburg, Germany\", \"Hessen, Kassel\"))\n",
      "        location = (location.replace(\",\", \"\")).lower()   #remove separator and normalize to lowercase\n",
      "        for e in removeWords:   #remove Bundesland and/or Country\n",
      "            if e.lower() in location:\n",
      "                location = location.strip(e.lower())\n",
      "                location = location.strip()   #strip off white space\n",
      "        '''\n",
      "        location = AccountInfoList[e][0].lower()\n",
      "        idxLoc1 = location.find('/')          # strip off everything from '/' on to the right (e.g. 'Frankfurt/M')\n",
      "        idxLoc2 = location.find('-')          # strip off everything from '-' on to the right (e.g. 'Duisburg-Essen')\n",
      "        if idxLoc1 != -1:\n",
      "            location = location[:idxLoc1]\n",
      "        if idxLoc2 != -1:\n",
      "            location = location[:idxLoc2]\n",
      "        if 'sporths' in location:\n",
      "            location = location.strip('sporths')   # the lib of K\u00f6lnSportHS has given that as their location!\n",
      "         \n",
      "        \n",
      "        #calculating Tweets per day and year\n",
      "        t0 = time.mktime(time.strptime(tweetsSince, \"%a %b %d %H:%M:%S +0000 %Y\"))#returns date in seconds (from 1970-01-01)\n",
      "        t1 = time.time() #returns current date in seconds (from 1970-01-01)\n",
      "        diff = int(round((t1 - t0)/86400)) #calculates the difference in days (86400 sec per day)\n",
      "        tweetsPerDay = round((float(nrOfStatusUpdates)/diff),2)   #returns nr of Tweets per day as a float\n",
      "        diffYear = round((diff/365.0),2)\n",
      "        tweetsPerYear = round((float(nrOfStatusUpdates)/diffYear),2)   #returns nr of Tweets per year as a float\n",
      "        \n",
      "        #calculating time since last Tweet\n",
      "        LastTweet_t0 = time.mktime(time.strptime(DateOfLastTweet, \"%a %b %d %H:%M:%S +0000 %Y\"))\n",
      "        daysSinceLastTweet = int(round((t1 - LastTweet_t0)/86400))\n",
      "        \n",
      "        #writing to the dictionary\n",
      "        newDict['screen_name'] = screenName\n",
      "        newDict['id_str'] = UserID\n",
      "        newDict['location'] = location\n",
      "        newDict['followers_count'] = nrOfFollowers\n",
      "        newDict['friends_count'] = nrOfFriends\n",
      "        newDict['statuses_count'] = nrOfStatusUpdates\n",
      "        newDict['created_at'] = tweetsSince\n",
      "        newDict['created_at_sec'] = t0\n",
      "        newDict['days'] = diff\n",
      "        newDict['tweets_per_day'] = tweetsPerDay\n",
      "        newDict['tweets_per_year'] = tweetsPerYear\n",
      "        newDict['days_since_last_tweet'] = daysSinceLastTweet\n",
      "        baseStatsList.append(newDict) #writing to the List\n",
      "        \n",
      "    return baseStatsList\n",
      "\n",
      "\n",
      "\n",
      "########################################\n",
      "#                                      #\n",
      "#  Function for the reporting section  #\n",
      "#                                      #\n",
      "########################################\n",
      "\n",
      "\n",
      "#return the median of Tweets per Day\n",
      "def medianOfTPD(LoD):\n",
      "    l = []\n",
      "    for e in LoD:\n",
      "        l.append(e['tweets_per_day'])\n",
      "    l.sort()\n",
      "    if len(l)%2 != 0:\n",
      "        median = l[len(l)/2]\n",
      "    else:\n",
      "        median = (l[len(l)/2-1] + l[len(l)/2])/2.0\n",
      "    return median\n",
      "\n",
      "\n",
      "#Sorting the Accounts based on created_at\n",
      "def sortingDate(L):\n",
      "    '''\n",
      "    input = baseStats(StatusLists) or list of dicts\n",
      "    output = sorted list of dicts from oldest to newest account\n",
      "    '''\n",
      "    l=L[:]\n",
      "    l.sort(key=operator.itemgetter('created_at_sec'))\n",
      "    return l\n",
      "\n",
      "\n",
      "#Sorting the Accounts based on days_since_last_tweet\n",
      "def sortingDateOfLastTweet(L):\n",
      "    '''\n",
      "    input = baseStats(StatusLists) or list of dicts\n",
      "    output = sorted list of dicts from oldest to newest account\n",
      "    '''\n",
      "    l=L[:]\n",
      "    l.sort(key=operator.itemgetter('days_since_last_tweet'))\n",
      "    return l\n",
      "\n",
      "#get the inactive accounts (i.e. accounts without a Tweet in the last 100 days\n",
      "def getInactiveAccounts(sortingDateOfLastTweet):\n",
      "    l = []\n",
      "    for e in sortingDateOfLastTweet:\n",
      "        if e['days_since_last_tweet'] > 100:\n",
      "            l.append(e['screen_name'])\n",
      "    if len(l) == 0:\n",
      "        print 'There is no inactive library in this group. (I.e. all libraries have tweeted in the last 100 days.)'\n",
      "    elif len(l) == 1:\n",
      "        print l[0], \"hasn't tweeted in the last 100 days. This library can be considered inactive on Twitter.\" \n",
      "    else:\n",
      "        s = \", \".join(l)\n",
      "        print s, \"haven't tweeted in the last 100 days. These libraries can be considered inactive on Twitter.\" \n",
      "\n",
      "\n",
      "        \n",
      "#Sorting the Accounts based on number of Tweets\n",
      "def sortingTweets(L):\n",
      "    '''\n",
      "    input = baseStatsList(StatusLists) or list of dicts\n",
      "    output = sorted list of dicts from lousiest Tweeter to SocialMedia Addict\n",
      "    '''\n",
      "    li=L[:]\n",
      "    import operator\n",
      "    li.sort(key=operator.itemgetter('statuses_count'))\n",
      "    return li\n",
      "        \n",
      "#Printing a summary sorted by date\n",
      "def printSummary(dictList):\n",
      "    '''a small function to print a summary of the list of dicts sorted by date'''\n",
      "    for e in range(len(dictList)):\n",
      "        print dictList[e]['location'], ':', dictList[e]['screen_name'], '= UserID:', dictList[e]['id_str']\n",
      "        print '--> Followers:', dictList[e]['followers_count'], '; Following:', dictList[e]['friends_count'], '; Tweets:', dictList[e]['statuses_count']\n",
      "        print '--> Tweets since:', dictList[e]['created_at'][4:7], dictList[e]['created_at'][-4:], '=', dictList[e]['days'], 'days', '; Tweets per day:', dictList[e]['tweets_per_day']\n",
      "        print\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Requesting Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. National Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1: get the list of screennames\n",
      "# ==> insert csv-name  !!\n",
      "NatBib_libList = getLoSN('NatBibTwitter.csv')\n",
      "print len(NatBib_libList), 'libraries were data mined'\n",
      "\n",
      "# 2: get the account information for each screenname\n",
      "NatBib_accountInfoList = AccountInfo(NatBib_libList)\n",
      "\n",
      "# 3: get some basic stats and write them to a list of dictionaries\n",
      "NatBib_baseStatsList = baseStats(NatBib_accountInfoList)\n",
      "\n",
      "# 4: save this LoD as a csv to the cwd\n",
      "# ==> insert csv-name  !!\n",
      "exp2CSV(NatBib_baseStatsList, 'NatBib_BasicStats.csv')\n",
      "print 'The findings were saved as a CSV file to your cwd as NatBib_BasicStats_[current datestamp].csv'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 libraries were data mined\n",
        "The findings were saved as a CSV file to your cwd as NatBib_BasicStats_[current datestamp].csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. University Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1: get the list of screennames\n",
      "# ==> insert csv-name  !!\n",
      "UniBib_libList = getLoSN('UniBibTwitter.csv')\n",
      "print len(UniBib_libList), 'libraries were data mined'\n",
      "\n",
      "# 2: get the account information for each screenname\n",
      "UniBib_accountInfoList = AccountInfo(UniBib_libList)\n",
      "\n",
      "# 3: get some basic stats and write them to a list of dictionaries\n",
      "UniBib_baseStatsList = baseStats(UniBib_accountInfoList)\n",
      "\n",
      "# 4: save this LoD as a csv to the cwd\n",
      "# ==> insert csv-name  !!\n",
      "exp2CSV(UniBib_baseStatsList, 'UniBib_BasicStats.csv')\n",
      "print 'The findings were saved as a CSV file to your cwd as UniBib_BasicStats_[current datestamp].csv.'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "27 libraries were data mined\n",
        "The findings were saved as a CSV file to your cwd as UniBib_BasicStats_[current datestamp].csv."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Public Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1: get the list of screennames\n",
      "# ==> insert csv-name  !!\n",
      "OeBib_libList = getLoSN('OeBibTwitter.csv')\n",
      "print len(OeBib_libList), 'libraries were queried.'\n",
      "\n",
      "# 2: get the account information for each screenname\n",
      "OeBib_accountInfoList = AccountInfo(OeBib_libList)\n",
      "\n",
      "# 3: get some basic stats and write them to a list of dictionaries\n",
      "OeBib_baseStatsList = baseStats(OeBib_accountInfoList)\n",
      "\n",
      "# 4: save this LoD as a csv to the cwd\n",
      "# ==> insert csv-name  !!\n",
      "exp2CSV(OeBib_baseStatsList, 'OeBib_BasicStats.csv')\n",
      "print 'The findings were saved as a CSV file to your cwd as OeBib_BasicStats_[current datestamp].csv.'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21 libraries were queried.\n",
        "The findings were saved as a CSV file to your cwd as OeBib_BasicStats_[current datestamp].csv."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Report"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "National Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NatBib_median = medianOfTPD(NatBib_baseStatsList)\n",
      "\n",
      "NatBib_dateSortList = sortingDate(NatBib_baseStatsList)\n",
      "\n",
      "NatBib_tweetSortList = sortingTweets(NatBib_baseStatsList)\n",
      "\n",
      "#--------\n",
      "\n",
      "print 'There are', len(NatBib_libList), 'libraries in this category.'\n",
      "print\n",
      "print 'Taken the median, on average these libraries send about', NatBib_median, 'Tweets per day.'\n",
      "print\n",
      "print 'Oldest account:', NatBib_dateSortList[0]['screen_name'], 'with', NatBib_dateSortList[0]['tweets_per_day'], 'Tweets per day.' \n",
      "print 'Latest account:', NatBib_dateSortList[-1]['screen_name'], 'with', NatBib_dateSortList[-1]['tweets_per_day'], 'Tweets per day.' \n",
      "print\n",
      "getInactiveAccounts(NatBib_baseStatsList)\n",
      "print\n",
      "print 'Lousiest Tweeter:', NatBib_tweetSortList[0]['screen_name'], 'with', NatBib_tweetSortList[0]['statuses_count'], 'Tweets.' \n",
      "print 'SocialMedia Addict:', NatBib_tweetSortList[-1]['screen_name'], 'with', NatBib_tweetSortList[-1]['statuses_count'], 'Tweets.' \n",
      "print\n",
      "printSummary(NatBib_dateSortList)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 3 libraries in this category.\n",
        "\n",
        "Taken the median, on average these libraries send about 0.7 Tweets per day.\n",
        "\n",
        "Oldest account: bsb_muenchen with 0.7 Tweets per day.\n",
        "Latest account: dnb_aktuelles with 0.75 Tweets per day.\n",
        "\n",
        "There is no inactive library in this group. (I.e. all libraries have tweeted in the last 100 days.)\n",
        "\n",
        "Lousiest Tweeter: dnb_aktuelles with 464 Tweets.\n",
        "SocialMedia Addict: bsb_muenchen with 1256 Tweets.\n",
        "\n",
        "m\u00fcnchen : bsb_muenchen = UserID: 39468408\n",
        "--> Followers: 1727 ; Following: 95 ; Tweets: 1256\n",
        "--> Tweets since: May 2009 = 1791 days ; Tweets per day: 0.7\n",
        "\n",
        "berlin : sbb_news = UserID: 47622879\n",
        "--> Followers: 1313 ; Following: 0 ; Tweets: 844\n",
        "--> Tweets since: Jun 2009 = 1756 days ; Tweets per day: 0.48\n",
        "\n",
        "frankfurt : dnb_aktuelles = UserID: 714103813\n",
        "--> Followers: 558 ; Following: 85 ; Tweets: 464\n",
        "--> Tweets since: Jul 2012 = 622 days ; Tweets per day: 0.75\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "University Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "UniBib_median = medianOfTPD(UniBib_baseStatsList)\n",
      "\n",
      "UniBib_dateSortList = sortingDate(UniBib_baseStatsList)\n",
      "\n",
      "UniBib_tweetSortList = sortingTweets(UniBib_baseStatsList)\n",
      "\n",
      "\n",
      "#--------\n",
      "\n",
      "print 'There are', len(UniBib_libList), 'libraries in this category.'\n",
      "print\n",
      "print 'Taken the median, on average these libraries send about', UniBib_median, 'Tweets per day.'\n",
      "print\n",
      "print 'Oldest account:', UniBib_dateSortList[0]['screen_name'], 'with', UniBib_dateSortList[0]['tweets_per_day'], 'Tweets per day.' \n",
      "print 'Latest account:', UniBib_dateSortList[-1]['screen_name'], 'with', UniBib_dateSortList[-1]['tweets_per_day'], 'Tweets per day.' \n",
      "print\n",
      "getInactiveAccounts(UniBib_baseStatsList)\n",
      "print\n",
      "print 'Lousiest Tweeter:', UniBib_tweetSortList[0]['screen_name'], 'with', UniBib_tweetSortList[0]['statuses_count'], 'Tweets.' \n",
      "print 'SocialMedia Addict:', UniBib_tweetSortList[-1]['screen_name'], 'with', UniBib_tweetSortList[-1]['statuses_count'], 'Tweets.' \n",
      "print\n",
      "printSummary(UniBib_dateSortList)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 27 libraries in this category.\n",
        "\n",
        "Taken the median, on average these libraries send about 0.41 Tweets per day.\n",
        "\n",
        "Oldest account: elibbremen with 1.13 Tweets per day.\n",
        "Latest account: kizuulm with 0.13 Tweets per day.\n",
        "\n",
        "ub_oldenburg, ubbayreuth_info, zbsport haven't tweeted in the last 100 days. These libraries can be considered inactive on Twitter.\n",
        "\n",
        "Lousiest Tweeter: ubbayreuth_info with 23 Tweets.\n",
        "SocialMedia Addict: stabihh with 5546 Tweets.\n",
        "\n",
        "bremen : elibbremen = UserID: 20671215\n",
        "--> Followers: 1079 ; Following: 755 ; Tweets: 2129\n",
        "--> Tweets since: Feb 2009 = 1880 days ; Tweets per day: 1.13\n",
        "\n",
        "hannover : tibub = UserID: 22807586\n",
        "--> Followers: 1571 ; Following: 1025 ; Tweets: 2143\n",
        "--> Tweets since: Mar 2009 = 1860 days ; Tweets per day: 1.15\n",
        "\n",
        "hamburg : tubhh = UserID: 25095207\n",
        "--> Followers: 653 ; Following: 30 ; Tweets: 1022\n",
        "--> Tweets since: Mar 2009 = 1846 days ; Tweets per day: 0.55\n",
        "\n",
        "k\u00f6ln : zbsport = UserID: 25307940\n",
        "--> Followers: 503 ; Following: 2 ; Tweets: 58\n",
        "--> Tweets since: Mar 2009 = 1845 days ; Tweets per day: 0.03\n",
        "\n",
        "bayreuth : ubbayreuth_info = UserID: 25879034\n",
        "--> Followers: 354 ; Following: 0 ; Tweets: 23\n",
        "--> Tweets since: Mar 2009 = 1842 days ; Tweets per day: 0.01\n",
        "\n",
        "dortmund : unibib = UserID: 38386894\n",
        "--> Followers: 1067 ; Following: 12 ; Tweets: 358\n",
        "--> Tweets since: May 2009 = 1797 days ; Tweets per day: 0.2\n",
        "\n",
        "oldenburg : ub_oldenburg = UserID: 39998615\n",
        "--> Followers: 164 ; Following: 27 ; Tweets: 50\n",
        "--> Tweets since: May 2009 = 1789 days ; Tweets per day: 0.03\n",
        "\n",
        "dresden : slubdresden = UserID: 40217644\n",
        "--> Followers: 3817 ; Following: 826 ; Tweets: 2254\n",
        "--> Tweets since: May 2009 = 1788 days ; Tweets per day: 1.26\n",
        "\n",
        "bochum : ubbochum = UserID: 41152402\n",
        "--> Followers: 1070 ; Following: 218 ; Tweets: 1039\n",
        "--> Tweets since: May 2009 = 1784 days ; Tweets per day: 0.58\n",
        "\n",
        "berlin : ubhumboldtuni = UserID: 47903084\n",
        "--> Followers: 488 ; Following: 25 ; Tweets: 603\n",
        "--> Tweets since: Jun 2009 = 1755 days ; Tweets per day: 0.34\n",
        "\n",
        "hamburg : stabihh = UserID: 52349626\n",
        "--> Followers: 1730 ; Following: 250 ; Tweets: 5546\n",
        "--> Tweets since: Jun 2009 = 1742 days ; Tweets per day: 3.18\n",
        "\n",
        "g\u00f6ttingen : subugoe = UserID: 67543866\n",
        "--> Followers: 182 ; Following: 48 ; Tweets: 243\n",
        "--> Tweets since: Aug 2009 = 1691 days ; Tweets per day: 0.14\n",
        "\n",
        "hamburg : hsubib = UserID: 74455176\n",
        "--> Followers: 699 ; Following: 166 ; Tweets: 1374\n",
        "--> Tweets since: Sep 2009 = 1665 days ; Tweets per day: 0.83\n",
        "\n",
        "leipzig : ubleipzig = UserID: 76315366\n",
        "--> Followers: 1163 ; Following: 232 ; Tweets: 632\n",
        "--> Tweets since: Sep 2009 = 1658 days ; Tweets per day: 0.38\n",
        "\n",
        "duisburg : ubdue = UserID: 86322871\n",
        "--> Followers: 797 ; Following: 34 ; Tweets: 887\n",
        "--> Tweets since: Oct 2009 = 1620 days ; Tweets per day: 0.55\n",
        "\n",
        "regensburg : ubreg = UserID: 87673073\n",
        "--> Followers: 750 ; Following: 254 ; Tweets: 1274\n",
        "--> Tweets since: Nov 2009 = 1614 days ; Tweets per day: 0.79\n",
        "\n",
        "berlin : ub_tu_berlin = UserID: 105141688\n",
        "--> Followers: 879 ; Following: 377 ; Tweets: 740\n",
        "--> Tweets since: Jan 2010 = 1543 days ; Tweets per day: 0.48\n",
        "\n",
        "kassel : ubkassel = UserID: 140830161\n",
        "--> Followers: 172 ; Following: 136 ; Tweets: 258\n",
        "--> Tweets since: May 2010 = 1432 days ; Tweets per day: 0.18\n",
        "\n",
        "w\u00fcrzburg : ub_wue = UserID: 164600181\n",
        "--> Followers: 220 ; Following: 17 ; Tweets: 406\n",
        "--> Tweets since: Jul 2010 = 1369 days ; Tweets per day: 0.3\n",
        "\n",
        "bonn : ulbbonn = UserID: 166924284\n",
        "--> Followers: 411 ; Following: 36 ; Tweets: 596\n",
        "--> Tweets since: Jul 2010 = 1362 days ; Tweets per day: 0.44\n",
        "\n",
        "braunschweig : unibib_bs = UserID: 190994679\n",
        "--> Followers: 123 ; Following: 1 ; Tweets: 456\n",
        "--> Tweets since: Sep 2010 = 1300 days ; Tweets per day: 0.35\n",
        "\n",
        "erlangen : ub_fau = UserID: 196916415\n",
        "--> Followers: 431 ; Following: 24 ; Tweets: 528\n",
        "--> Tweets since: Sep 2010 = 1286 days ; Tweets per day: 0.41\n",
        "\n",
        "marburg : unibib_mr = UserID: 418611112\n",
        "--> Followers: 307 ; Following: 49 ; Tweets: 257\n",
        "--> Tweets since: Nov 2011 = 867 days ; Tweets per day: 0.3\n",
        "\n",
        "bielefeld : ub_bi = UserID: 508565664\n",
        "--> Followers: 274 ; Following: 239 ; Tweets: 812\n",
        "--> Tweets since: Feb 2012 = 768 days ; Tweets per day: 1.06\n",
        "\n",
        "mainz : ubmainz = UserID: 602631712\n",
        "--> Followers: 654 ; Following: 1068 ; Tweets: 2343\n",
        "--> Tweets since: Jun 2012 = 668 days ; Tweets per day: 3.51\n",
        "\n",
        "karlsruhe : kitbibliothek = UserID: 607258639\n",
        "--> Followers: 269 ; Following: 20 ; Tweets: 242\n",
        "--> Tweets since: Jun 2012 = 663 days ; Tweets per day: 0.37\n",
        "\n",
        "ulm : kizuulm = UserID: 1222219579\n",
        "--> Followers: 143 ; Following: 64 ; Tweets: 54\n",
        "--> Tweets since: Feb 2013 = 405 days ; Tweets per day: 0.13\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Public Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "OeBib_median = medianOfTPD(OeBib_baseStatsList)\n",
      "\n",
      "OeBib_dateSortList = sortingDate(OeBib_baseStatsList)\n",
      "\n",
      "OeBib_tweetSortList = sortingTweets(OeBib_baseStatsList)\n",
      "\n",
      "\n",
      "#--------\n",
      "\n",
      "print 'There are', len(OeBib_libList), 'libraries in this category.'\n",
      "print\n",
      "print 'Taken the median, on average these libraries send about', OeBib_median, 'Tweets per day.'\n",
      "print\n",
      "print 'Oldest account:', OeBib_dateSortList[0]['screen_name'], 'with', OeBib_dateSortList[0]['tweets_per_day'], 'Tweets per day.' \n",
      "print 'Latest account:', OeBib_dateSortList[-1]['screen_name'], 'with', OeBib_dateSortList[-1]['tweets_per_day'], 'Tweets per day.' \n",
      "print\n",
      "getInactiveAccounts(OeBib_baseStatsList)\n",
      "print\n",
      "print 'Lousiest Tweeter:', OeBib_tweetSortList[0]['screen_name'], 'with', OeBib_tweetSortList[0]['statuses_count'], 'Tweets.' \n",
      "print 'SocialMedia Addict:', OeBib_tweetSortList[-1]['screen_name'], 'with', OeBib_tweetSortList[-1]['statuses_count'], 'Tweets.' \n",
      "print\n",
      "printSummary(OeBib_dateSortList)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 21 libraries in this category.\n",
        "\n",
        "Taken the median, on average these libraries send about 1.09 Tweets per day.\n",
        "\n",
        "Oldest account: stabuewuerzburg with 2.46 Tweets per day.\n",
        "Latest account: bibliothek_wit with 0.61 Tweets per day.\n",
        "\n",
        "buecherei_ms hasn't tweeted in the last 100 days. This library can be considered inactive on Twitter.\n",
        "\n",
        "Lousiest Tweeter: buecherei_ms with 95 Tweets.\n",
        "SocialMedia Addict: stbibkoeln with 9609 Tweets.\n",
        "\n",
        "w\u00fcrzburg : stabuewuerzburg = UserID: 15178997\n",
        "--> Followers: 413 ; Following: 194 ; Tweets: 5208\n",
        "--> Tweets since: Jun 2008 = 2117 days ; Tweets per day: 2.46\n",
        "\n",
        "erlangen : stabi_erlangen = UserID: 21177090\n",
        "--> Followers: 988 ; Following: 497 ; Tweets: 6750\n",
        "--> Tweets since: Feb 2009 = 1875 days ; Tweets per day: 3.6\n",
        "\n",
        "berlin : stadtbibliothek = UserID: 32955209\n",
        "--> Followers: 283 ; Following: 17 ; Tweets: 420\n",
        "--> Tweets since: Apr 2009 = 1815 days ; Tweets per day: 0.23\n",
        "\n",
        "solingen : stabiso = UserID: 40726549\n",
        "--> Followers: 327 ; Following: 28 ; Tweets: 2227\n",
        "--> Tweets since: May 2009 = 1786 days ; Tweets per day: 1.25\n",
        "\n",
        "bremen : stabi_bremen = UserID: 46082445\n",
        "--> Followers: 981 ; Following: 32 ; Tweets: 242\n",
        "--> Tweets since: Jun 2009 = 1762 days ; Tweets per day: 0.14\n",
        "\n",
        "hamburg : hoeb4u = UserID: 47645067\n",
        "--> Followers: 210 ; Following: 131 ; Tweets: 5021\n",
        "--> Tweets since: Jun 2009 = 1756 days ; Tweets per day: 2.86\n",
        "\n",
        "chemnitz : sbchemnitz = UserID: 63668003\n",
        "--> Followers: 1103 ; Following: 1021 ; Tweets: 1122\n",
        "--> Tweets since: Aug 2009 = 1705 days ; Tweets per day: 0.66\n",
        "\n",
        "g\u00f6ttingen : stabigoe = UserID: 67596714\n",
        "--> Followers: 333 ; Following: 91 ; Tweets: 2301\n",
        "--> Tweets since: Aug 2009 = 1690 days ; Tweets per day: 1.36\n",
        "\n",
        "freiburg : stabifr = UserID: 69244340\n",
        "--> Followers: 623 ; Following: 394 ; Tweets: 2058\n",
        "--> Tweets since: Aug 2009 = 1684 days ; Tweets per day: 1.22\n",
        "\n",
        "neuss : stbneuss = UserID: 88621648\n",
        "--> Followers: 397 ; Following: 585 ; Tweets: 1575\n",
        "--> Tweets since: Nov 2009 = 1610 days ; Tweets per day: 0.98\n",
        "\n",
        "krefeld : mediothek = UserID: 107376250\n",
        "--> Followers: 668 ; Following: 850 ; Tweets: 2731\n",
        "--> Tweets since: Jan 2010 = 1536 days ; Tweets per day: 1.78\n",
        "\n",
        "g\u00fctersloh : stabiguetersloh = UserID: 118645887\n",
        "--> Followers: 445 ; Following: 169 ; Tweets: 550\n",
        "--> Tweets since: Mar 2010 = 1498 days ; Tweets per day: 0.37\n",
        "\n",
        "k\u00f6ln : stbibkoeln = UserID: 130538657\n",
        "--> Followers: 2120 ; Following: 1066 ; Tweets: 9609\n",
        "--> Tweets since: Apr 2010 = 1461 days ; Tweets per day: 6.58\n",
        "\n",
        "mannheim : stabi_mannheim = UserID: 139636927\n",
        "--> Followers: 471 ; Following: 265 ; Tweets: 841\n",
        "--> Tweets since: May 2010 = 1436 days ; Tweets per day: 0.59\n",
        "\n",
        "bielefeld : stb_bielefeld = UserID: 150604211\n",
        "--> Followers: 358 ; Following: 15 ; Tweets: 2448\n",
        "--> Tweets since: Jun 2010 = 1406 days ; Tweets per day: 1.74\n",
        "\n",
        "essen : stbessen = UserID: 180672165\n",
        "--> Followers: 357 ; Following: 77 ; Tweets: 832\n",
        "--> Tweets since: Aug 2010 = 1327 days ; Tweets per day: 0.63\n",
        "\n",
        "salzgitter : stbsalzgitter = UserID: 357509606\n",
        "--> Followers: 228 ; Following: 192 ; Tweets: 1024\n",
        "--> Tweets since: Aug 2011 = 963 days ; Tweets per day: 1.06\n",
        "\n",
        "m\u00fcnster : buecherei_ms = UserID: 377233907\n",
        "--> Followers: 108 ; Following: 3 ; Tweets: 95\n",
        "--> Tweets since: Sep 2011 = 930 days ; Tweets per day: 0.1\n",
        "\n",
        "m\u00f6nchengladbach : stadtbibmg = UserID: 417924693\n",
        "--> Followers: 320 ; Following: 703 ; Tweets: 942\n",
        "--> Tweets since: Nov 2011 = 868 days ; Tweets per day: 1.09\n",
        "\n",
        "d\u00fcsseldorf : stadtbueduedorf = UserID: 934766042\n",
        "--> Followers: 433 ; Following: 921 ; Tweets: 669\n",
        "--> Tweets since: Nov 2012 = 515 days ; Tweets per day: 1.3\n",
        "\n",
        "witten : bibliothek_wit = UserID: 1617127052\n",
        "--> Followers: 51 ; Following: 50 ; Tweets: 158\n",
        "--> Tweets since: Jul 2013 = 258 days ; Tweets per day: 0.61\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}