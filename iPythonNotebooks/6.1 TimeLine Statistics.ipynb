{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analyzing the Timeline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this Notebook, the timeline of each account is being analyzed:\n",
      "\n",
      "The Function `analyzeTimeline` takes as input the BibTwitter file for each library group and the timestamp of the timeline.json-files. The function returns an csv file for each library group (NatBib\\_timeLineStats\\_[datestamp].csv).\n",
      "\n",
      "The file contains one observation for each tweet with information in these categories:\n",
      "\n",
      "   - identification: keys 'screen_name', 'id_str'\n",
      "   - date: keys 'created_at', 'tweet_time', 'tweet_weekday', 'tweet_month'\n",
      "   - type: keys 'genuine_tweet', 'auto_tweet'\n",
      "   - resonance: keys 'favorite_count', , 'rt_count', 'resonance_Factor'  (the resonance factor is the sum of rt_count x 1.0 + favorite_count * 0.5)\n",
      "   - content: keys 'has_media', 'has_mention', 'has_url', 'has_hashtag'.\n",
      "   \n",
      "The file will be extended in Notebook 7.2 Communication with the category \n",
      "\n",
      "   - communication: keys 'is_reply', 'original_is_question', 'reply_is_answer', 'hours_to_answer', 'is_follower', 'follower_local', 'orphan'.\n",
      "   \n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from collections import Counter\n",
      "#import json\n",
      "\n",
      "def readJSON(filename):\n",
      "    import json\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "        return data\n",
      "\n",
      "import csv\n",
      "\n",
      "def impCSV(input_file):\n",
      "    '''\n",
      "    input_file = csv with keys: \"URL\", \"Twitter\"\n",
      "    output = list of dictionaries\n",
      "    '''\n",
      "    f = open(input_file, 'r')\n",
      "    d = csv.DictReader(f)\n",
      "    LoD = []   # list of dictionaries\n",
      "    for row in d:\n",
      "        LoD.append(row)\n",
      "    f.close()\n",
      "    return LoD\n",
      "    \n",
      "    \n",
      "def exp2CSV(listOfDict, filename):\n",
      "    '''\n",
      "    arguments = list of dictionaries, filename\n",
      "    output = saves file to cwd (current working directory)\n",
      "    '''\n",
      "    #creating the filename of the csv with current datestamp \n",
      "    import datetime\n",
      "    datestamp = datetime.datetime.now().strftime('%Y-%m-%d')    \n",
      "    outputfile = filename + '_' + datestamp + '.csv'\n",
      "    keyz = listOfDict[0].keys()\n",
      "    f = open(outputfile,'w')\n",
      "    dict_writer = csv.DictWriter(f,keyz)\n",
      "    dict_writer.writer.writerow(keyz)\n",
      "    dict_writer.writerows(listOfDict)\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Function Definition"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyzeTimeline(bibTwitterfile,timeStamp):\n",
      "    '''\n",
      "    input: filename of NatBibTwitter.csv, OeBibTwitter.csv or UniBibTwitter.csv and\n",
      "    Timestamp = Timestamp of the timeline files\n",
      "    output: saves a LoD as a csv of the timeline stats\n",
      "    '''\n",
      "\n",
      "    # open UniBibTwitter.csv\n",
      "    f = impCSV(bibTwitterfile)\n",
      "    l = []                                      # list of screen_names\n",
      "    LoD = []                                    # List of Dictionary for entire library group\n",
      "\n",
      "    for i in f:                                 # create list of screen_names/twitter handles\n",
      "        l.append(i['Twitter'])\n",
      "    \n",
      "    for handle in l:\n",
      "        lod = []                                # list of dictionary for each library\n",
      "        filename = handle + '_timeline_' + timeStamp + '.json'\n",
      "        tl = readJSON(filename)\n",
      "        for e in tl:\n",
      "            d = {}\n",
      "            \n",
      "            # 1. get date\n",
      "            #\n",
      "            t = e['created_at']\n",
      "            t0 = t.find(' ')                    # separates Weekday from Month\n",
      "            t1 = t.find(' ', t0+1)              # separates Month from day\n",
      "            t2 = t.find(' ', t1+1)              # separates day from time\n",
      "            t3 = t.find(':', t2+1)              # separates hour from minute\n",
      "    \n",
      "            weekday = t[:t0]\n",
      "            wd = {'Mon':1, 'Tue':2, 'Wed':3, 'Thu':4, 'Fri':5, 'Sat': 6, 'Sun':7}\n",
      "            if weekday in wd:\n",
      "                weekday = wd[weekday]\n",
      "            month_day = t[t0+1:t1]     \n",
      "            mon = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun': 6, 'Jul':7, 'Aug':8, 'Sep':9, \n",
      "                   'Oct':10, 'Nov':11, 'Dec':12}\n",
      "            if month_day in mon:\n",
      "                month_day = mon[month_day]\n",
      "            hour = t[t2+1:t3]\n",
      "            \n",
      "            # 2. RT?\n",
      "            #\n",
      "            if 'RT' in e['text'] or 'MT' in e['text']:      # just filter out RT and MT, keep 'via @' though, since these tweets often are considerably altered\n",
      "                rt = 0                           # if it's a genuine tweet of the library: 1, else: 0\n",
      "                rt_count = 'NA'                  # don't count the favs & RTs of tweets the library retweeted!\n",
      "                fav_count = 'NA'\n",
      "                resFac = 'NA'                    # don't count the tweet content analysis\n",
      "                hashtag = 'NA'\n",
      "                mention = 'NA'\n",
      "                URL = 'NA'\n",
      "                media = 'NA'\n",
      "            else:\n",
      "                rt = 1\n",
      "                \n",
      "                # 3. get retweet- & favorite count & calculate resonance factor\n",
      "                #\n",
      "                if type(e['retweet_count']) == int:          # \"Number of times this Tweet has been retweeted.\" \n",
      "                    rt_count = e['retweet_count']            # \"This field is __no longer__ capped at 99 and will not turn into a String for '100+'\"   \n",
      "                elif e['retweet_count'] == '100+':\n",
      "                    rt_count = 100\n",
      "                else:\n",
      "                    rt_count = 0\n",
      "                fav_count = e['favorite_count']              # \"Indicates approximately how many times this Tweet has been \"favorited\" by Twitter users.\"\n",
      "                resFac = rt_count * 1.0 + fav_count * 0.5    # calculating the resonance factor (1.0 for RT, 0.5 for Favs)\n",
      "                             \n",
      "                # 4. Content Analysis\n",
      "                #\n",
      "                if e['entities']['hashtags'] != []:\n",
      "                    hashtag = 1\n",
      "                else:\n",
      "                    hashtag = 0\n",
      "                if e['entities']['user_mentions'] != []:\n",
      "                    mention = 1\n",
      "                else:\n",
      "                    mention = 0\n",
      "                if e['entities']['urls'] != []:\n",
      "                    URL = 1\n",
      "                else:\n",
      "                    URL = 0\n",
      "                try:\n",
      "                    m = e['entities']['media']\n",
      "                    media = 1\n",
      "                except:\n",
      "                    media = 0\n",
      "                \n",
      "                \n",
      "            # 5. Auto tweet?\n",
      "            #\n",
      "            autoTweetSource = ['twitterfeed', 'wp.com', 'wp-to-twitter', 'tumblr',\n",
      "                               'instagram', 'blogs.', 'SharePress', 'facebook', 'studivz', 'paper.li']\n",
      "            for v in autoTweetSource:\n",
      "                if v in e['source']:\n",
      "                    at = 1\n",
      "                    break\n",
      "                else:\n",
      "                    at = 0\n",
      "            \n",
      "            # 6. Communiacation Analysis\n",
      "            # is later imported from the ArchiveStats-Files\n",
      "                                \n",
      "            # 7. fill dictionary\n",
      "            #\n",
      "            d = {'screen_name': handle, 'id_str': e['id_str'], 'created_at': t, 'tweet_weekday': weekday, 'tweet_month': month_day,\n",
      "                 'tweet_time': hour, 'genuine_tweet': rt, 'auto_tweet': at, 'rt_count': rt_count, 'favorite_count': fav_count,\n",
      "                 'resonance_Factor': resFac, 'has_hashtag': hashtag, 'has_mention': mention, 'has_url': URL, 'has_media': media}\n",
      "\n",
      "            lod.append(d)\n",
      "        LoD += lod\n",
      "    outputfilename = bibTwitterfile[:-11] + '_timelineStats'\n",
      "    exp2CSV(LoD, outputfilename)\n",
      "    return LoD\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Function calls"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nattl = analyzeTimeline('NatBibTwitter2.csv', '2014-04-07')\n",
      "print len(nattl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2560\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ubtl = analyzeTimeline('UniBibTwitter2.csv', '2014-04-07')\n",
      "print len(ubtl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "23905\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "oebtl = analyzeTimeline('OeBibTwitter2.csv', '2014-04-07')\n",
      "print len(oebtl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "32940\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Controll Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#    \n",
      "#control functions to check from which sources a tweet was sent\n",
      "#\n",
      "\n",
      "\n",
      "#extracting the sources of the tweets\n",
      "def tweetSource(timeline):\n",
      "    tweet_source = [sources['source']\n",
      "                    for sources in timeline]   # List of all sources\n",
      "    countedSources = Counter(tweet_source) #type = class: Counter({u'xxx': 12, u'yyy': 10, ...}) descending ordered\n",
      "    countedSources = countedSources.most_common()\n",
      "    \n",
      "    #Printing in a table\n",
      "    # Code adapted from MTSW 2. Ed.\n",
      "    from prettytable import PrettyTable\n",
      "\n",
      "    ptLang = PrettyTable(field_names=['Source', 'Count'])\n",
      "    [ptLang.add_row(kv) for kv in countedSources]\n",
      "    ptLang.align['Source'], ptLang.align['Count'] = 'l', 'r'\n",
      "    print str(len(tweet_source)) + ' Tweets were analyzed.'\n",
      "    print ptLang\n",
      "\n",
      "    \n",
      "    \n",
      "def printTweetFromSource(timeline, source):\n",
      "    '''\n",
      "    input: timeline and a word from the source under consideration (e.g. 'wp.com,' or 'tweetfeed')\n",
      "    output: prints out the index of the tweet in the timeline and the text of the tweet\n",
      "    '''\n",
      "    for e in range(len(timeline)):\n",
      "        if source in timeline[e]['source']:\n",
      "            print e, timeline[e]['text']\n",
      "            print\n",
      "\n",
      "            \n",
      "            \n",
      "# getting the timeline of an account:\n",
      "timeline = readJSON(json-filename)      #  <===== enter the json filename\n",
      "tweetSource(timeline)\n",
      "printTweetFromSource(timeline, source)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}